# Project 2 â€“ Data Quality & Incremental Pipeline (Azure Databricks)

## ğŸ“Œ Overview
This project demonstrates a *production-style data engineering pipeline* built using *Azure Databricks, **PySpark, and **Delta Lake*.

The focus of this project is on:
â€¢â   â Incremental data ingestion
â€¢â   â Data quality validation
â€¢â   â Handling bad records
â€¢â   â Delta Lake upserts (MERGE)
â€¢â   â Pipeline orchestration

This simulates how real-world data platforms process continuously arriving data safely and efficiently.

---

## ğŸ— Architecture Overview

Azure Blob Storage (Raw Data)
â†“
Bronze Layer (Incremental Load)
â†“
Silver Layer (Data Quality Validation)
â†“
Silver Layer (Incremental MERGE / Upserts)
â†“
Gold Layer (Metrics & Monitoring)
â†“
Analytics / Reporting


All transformations are executed in *Azure Databricks using PySpark and Delta Lake*.

---

## ğŸ¥‰ Bronze Layer â€“ Incremental Ingestion

*Purpose:*  
Ingest only *new or updated records*, avoiding full reprocessing.

*Key Features:*
â€¢â   â Uses a watermark column (e.g. updated_at / ingestion_time)
â€¢â   â Filters only newly arrived data
â€¢â   â Appends data incrementally to Delta tables

*Why this matters:*  
Incremental ingestion improves performance, scalability, and cost efficiency in production pipelines.

---

## ğŸ¥ˆ Silver Layer â€“ Data Quality Validation

*Purpose:*  
Ensure that only *clean and reliable data* flows downstream.

*Data Quality Rules Implemented:*
â€¢â   â Not-null checks on critical columns
â€¢â   â Range validation (e.g. quantity > 0)
â€¢â   â Basic format checks
â€¢â   â Referential integrity checks

*Bad Data Handling:*
â€¢â   â Valid records â†’ stored in Silver tables
â€¢â   â Invalid records â†’ quarantined separately for analysis

This prevents bad data from corrupting downstream analytics.

---

## ğŸ”„ Silver Incremental Merge (Upserts)

*Purpose:*  
Handle late-arriving data and updates using *Delta Lake MERGE operations*.

*What this achieves:*
â€¢â   â Inserts new records
â€¢â   â Updates existing records
â€¢â   â Ensures idempotent pipeline runs

This mirrors real production systems where source data can change over time.

---

## ğŸ¥‡ Gold Layer â€“ Metrics & Monitoring

*Purpose:*  
Generate metrics to monitor pipeline health and data quality.

*Metrics Generated:*
â€¢â   â Total records processed
â€¢â   â Failed record count
â€¢â   â Failure percentage
â€¢â   â Daily ingestion summary

These metrics help data engineers monitor pipeline reliability and data trust.

---

## â± Pipeline Orchestration

The pipeline is orchestrated using *Databricks Jobs / notebook chaining*.

*Execution Order:*
1.â  â Bronze Incremental Load
2.â  â Silver Data Quality Validation
3.â  â Silver Incremental Merge
4.â  â Gold Metrics Generation

This ensures correct dependency management and fault isolation.

---

## ğŸ“‚ Project Structure

project-2-data-quality-incremental-pipeline/
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_bronze_incremental.py
â”‚ â”œâ”€â”€ 02_silver_data_quality.py
â”‚ â”œâ”€â”€ 03_silver_incremental_merge.py
â”‚ â””â”€â”€ 04_pipeline_orchestration.py
â”‚
â”œâ”€â”€ dq_rules/
â”‚ â””â”€â”€ dq_rules.yaml
â”‚
â”œâ”€â”€ screenshots/
â”‚ â”œâ”€â”€ incremental_load.jpeg
â”‚ â”œâ”€â”€ dq_failures.jpeg
â”‚ â””â”€â”€ orchestration.jpeg
â”‚
â””â”€â”€ README.md


---

## ğŸ›  Tech Stack
â€¢â   â Azure Databricks
â€¢â   â Apache Spark (PySpark)
â€¢â   â Delta Lake
â€¢â   â Azure Blob Storage
â€¢â   â GitHub

---

## ğŸ’¬ Interview Talking Points
â€¢â   â Why incremental ingestion is critical for scalable pipelines
â€¢â   â How data quality checks prevent downstream failures
â€¢â   â Difference between append and merge in Delta Lake
â€¢â   â Handling late-arriving and updated records
â€¢â   â Monitoring pipelines using quality metrics

---

## âœ… Project Status
âœ” Incremental Bronze Ingestion  
âœ” Data Quality Validation  
âœ” Delta Lake Upserts  
âœ” Pipeline Orchestration  
âœ” Monitoring Metrics  

This project represents a *real-world, production-style data engineering pipeline*.
